{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68a2990",
   "metadata": {},
   "source": [
    "## MODELS: latencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4031b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Ruta a la carpeta con modelos exportados\n",
    "projects_dir = Path(\"projects\")\n",
    "\n",
    "# Ruta a las im√°genes de prueba\n",
    "test_images = list(Path(\"data/test/images\").glob(\"*.jpg\"))[:50]\n",
    "if not test_images:\n",
    "    raise FileNotFoundError(\"No se encontraron im√°genes en 'data/test/images'\")\n",
    "\n",
    "print(f\"üì∑ Se usar√°n {len(test_images)} im√°genes para la evaluaci√≥n.\\n\")\n",
    "\n",
    "# Crear lista para guardar resultados\n",
    "results_list = []\n",
    "\n",
    "# Iterar sobre cada carpeta de modelo\n",
    "for model_dir in projects_dir.iterdir():\n",
    "    weights_path = model_dir / \"weights\" / \"best.pt\"\n",
    "    if not weights_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  No se encontr√≥ el modelo en: {weights_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üöÄ Evaluando modelo: {model_dir.name}\")\n",
    "\n",
    "    try:\n",
    "        model = YOLO(str(weights_path))\n",
    "\n",
    "        # Medir latencias\n",
    "        latencies = []\n",
    "        for img_path in test_images:\n",
    "            start = time.perf_counter()\n",
    "            _ = model(img_path)\n",
    "            end = time.perf_counter()\n",
    "            latencies.append((end - start) * 1000)  # en milisegundos\n",
    "\n",
    "        latency_p50 = np.percentile(latencies, 50)\n",
    "        latency_p95 = np.percentile(latencies, 95)\n",
    "        latency_mean = np.mean(latencies)\n",
    "\n",
    "        results_list.append({\n",
    "            \"model\": model_dir.name,\n",
    "            \"latency_p50_ms\": round(latency_p50, 2),\n",
    "            \"latency_p95_ms\": round(latency_p95, 2),\n",
    "            \"latency_mean_ms\": round(latency_mean, 2)\n",
    "        })\n",
    "\n",
    "        print(f\"   ‚úÖ Latencia P50: {latency_p50:.2f} ms\")\n",
    "        print(f\"   ‚úÖ Latencia P95: {latency_p95:.2f} ms\")\n",
    "        print(f\"   ‚úÖ Latencia media: {latency_mean:.2f} ms\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con el modelo {model_dir.name}: {e}\\n\")\n",
    "\n",
    "# Mostrar resultados al final\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results_list)\n",
    "df = df.sort_values(by=\"latency_p50_ms\")\n",
    "print(\"\\nüìä Resumen de latencias:\")\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b6dbe",
   "metadata": {},
   "source": [
    "## MODELS: val vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# ================================================================\n",
    "# CONFIGURACI√ìN\n",
    "# ================================================================\n",
    "projects_dir = Path(\"projects\")\n",
    "\n",
    "# Detectar data.yaml\n",
    "default_yaml = Path(\"data.yaml\")\n",
    "\n",
    "if default_yaml.exists():\n",
    "    data_yaml = str(default_yaml)\n",
    "else:\n",
    "    data_folder = Path(\"data\")\n",
    "    yaml_files = list(data_folder.glob(\"*.yaml\")) + list(data_folder.glob(\"*.yml\"))\n",
    "\n",
    "    if len(yaml_files) == 0:\n",
    "        raise FileNotFoundError(\"No se encontr√≥ 'data.yaml' ni ning√∫n archivo YAML dentro de 'data/'.\")\n",
    "    \n",
    "    data_yaml = str(yaml_files[0])\n",
    "    print(f\"‚ö†Ô∏è 'data.yaml' no encontrado. Usando: {data_yaml}\")\n",
    "\n",
    "# ================================================================\n",
    "# DETECTAR MODELOS AUTOM√ÅTICAMENTE\n",
    "# ================================================================\n",
    "model_paths = list(projects_dir.glob(\"*/weights/best.pt\"))\n",
    "\n",
    "if not model_paths:\n",
    "    raise FileNotFoundError(\"No se encontraron modelos en projects/*/weights/best.pt\")\n",
    "\n",
    "print(\"üì¶ Modelos detectados:\")\n",
    "for p in model_paths:\n",
    "    print(\"  -\", p)\n",
    "\n",
    "\n",
    "results_global = []  # Para el CSV final\n",
    "\n",
    "# ================================================================\n",
    "# PROCESAR CADA MODELO\n",
    "# ================================================================\n",
    "for model_path in model_paths:\n",
    "\n",
    "    model_name = model_path.parent.parent.name\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"üöÄ Evaluando modelo: {model_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    output_dir = Path(f\"results/{model_name}\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = YOLO(str(model_path))\n",
    "\n",
    "    # ------------------------ VAL SET ------------------------\n",
    "    print(\"üìä Ejecutando evaluaci√≥n en VAL set...\")\n",
    "    metrics_val = model.val(data=data_yaml, split=\"val\")\n",
    "    val_dict = metrics_val.results_dict\n",
    "    val_dict = {f\"val_{k.replace('metrics/', '').replace('(B)', '')}\": v for k, v in val_dict.items()}\n",
    "\n",
    "    # Guardar JSON de val\n",
    "    with open(output_dir / f\"{model_name}_val.json\", \"w\") as f:\n",
    "        json.dump(val_dict, f, indent=4)\n",
    "\n",
    "    # ------------------------ TEST SET ------------------------\n",
    "    print(\"üìä Ejecutando evaluaci√≥n en TEST set...\")\n",
    "    metrics_test = model.val(data=data_yaml, split=\"test\")\n",
    "    test_dict = metrics_test.results_dict\n",
    "    test_dict = {f\"test_{k.replace('metrics/', '').replace('(B)', '')}\": v for k, v in test_dict.items()}\n",
    "\n",
    "    # Guardar JSON de test\n",
    "    with open(output_dir / f\"{model_name}_test.json\", \"w\") as f:\n",
    "        json.dump(test_dict, f, indent=4)\n",
    "\n",
    "    # ------------------------ COMBINAR VAL + TEST ------------------------\n",
    "    combined = {\"model\": model_name}\n",
    "    combined.update(val_dict)\n",
    "    combined.update(test_dict)\n",
    "\n",
    "    # Guardar CSV del modelo\n",
    "    pd.DataFrame([combined]).to_csv(output_dir / f\"{model_name}_val_test.csv\", index=False)\n",
    "\n",
    "    # A√±adir al global\n",
    "    results_global.append(combined)\n",
    "\n",
    "\n",
    "# ================================================================\n",
    "# CREAR CSV GLOBAL\n",
    "# ================================================================\n",
    "df = pd.DataFrame(results_global)\n",
    "df = df.set_index(\"model\", drop=False)\n",
    "\n",
    "df.to_csv(\"results/VAL_TEST_COMPARISON.csv\", index=False)\n",
    "\n",
    "print(\"\\nüìå COMPARATIVA FINAL VAL vs TEST\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nüìÑ CSV global guardado en: results/VAL_TEST_COMPARISON.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
